{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting. ML approach.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this notebook, where we embark on an exploration of a Machine Learning approach to predict the number of cyberattacks a country may face in the following month. Supervised models can be used for time series, as long as we have a way to extract seasonality and put it into a variable. Examples include creating a variable for a year, a month, or a day of the week, etc. These are then used as the X variables in your supervised model and the ‘y’ is the actual value of the time series. You can also include lagged versions of y (the past value of y) into the X data, in order to add autocorrelation effects.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. Time Series Visualization\n",
    "\n",
    "2. Dataset Construction\n",
    "    - 2.1 Baseline\n",
    "    - 2.2 Lagged features\n",
    "    - 2.3 Rolling statistics feature\n",
    "\n",
    "3. Train models\n",
    "\n",
    "4. Evaluation\n",
    "\n",
    "5. Bonus Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Visualization\n",
    "\n",
    "Let us read the data and visualize it as a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df1 = pd.read_csv('../Data/21_november_to_april.csv')\n",
    "df2 = pd.read_csv('../Data/22_april_to_november.csv')\n",
    "df3 = pd.read_csv('../Data/22_november_to_april.csv')\n",
    "df4 = pd.read_csv('../Data/23_april_to_november.csv')\n",
    "\n",
    "# Concatenate dataframes\n",
    "df = pd.concat([df1, df2, df3, df4], axis=0, ignore_index=True)\n",
    "\n",
    "# Delete dataframes\n",
    "del  df1, df2, df3, df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some countries to analyze\n",
    "df_Spain = select_country(df, 'Spain')\n",
    "df_USA = select_country(df, 'United States')\n",
    "df_Singapore = select_country(df, 'Singapore')\n",
    "df_Germany = select_country(df, 'Germany')\n",
    "df_Japan = select_country(df, 'Japan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_Spain = visualize_ts(df_Spain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_USA = visualize_ts(df_USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_Singapore = visualize_ts(df_Singapore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_Germany = visualize_ts(df_Germany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_count_Japan = visualize_ts(df_Japan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "\n",
    "Let us generate features to forecast the number of cyberattacks a country might encounter in the future. We will begin by incorporating temporal elements such as the month, year, day, and so on. This will establish a baseline dataset for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline dataset. Just temporal information\n",
    "\n",
    "df_Spain = create_baseline_dataset(daily_count_Spain)\n",
    "df_USA = create_baseline_dataset(daily_count_USA)\n",
    "df_Singapore = create_baseline_dataset(daily_count_Singapore)\n",
    "df_Germany = create_baseline_dataset(daily_count_Germany)\n",
    "df_Japan = create_baseline_dataset(daily_count_Japan)\n",
    "\n",
    "# Visualize df_USA\n",
    "df_USA.head() # The first column is the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged feature\n",
    "\n",
    "A valuable feature for anticipating the number of attacks a country might experience in the future is the historical count of attacks. To forecast the number of attacks at a given time, say $t$, we can use information on the number of cyberattacks at an earlier time $t-i$, where $i\\geq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of lagged dataset\n",
    "add_lags(df_USA, 3, 'count').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling statistics features\n",
    "\n",
    "Additional valuable features that we can derive from the lagged variables include various statistics like the mean, maximum, minimum, and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of rolling dataset\n",
    "create_rolling_features(df_USA, 'count', windows=[2,3]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models\n",
    "\n",
    "Let us establish a routine for training various Machine Learning regression algorithms. The objective is to use historical data at time $t$ to make predictions for time $t+1$. With our dataset spanning two years, we plan to allocate one month for model validation and another month for testing.\n",
    "\n",
    "The training pipeline is as follows:\n",
    "\n",
    " - Choose a time series from a specific country.\n",
    "\n",
    " - Determine the number of lagged values and windows to generate the dataset from the chosen time series. This will serve as a hyperparameter that requires tuning, with various combinations tested to identify the optimal configuration.\n",
    "\n",
    " - Train multiple regression models while conducting hyperparameter tuning on each using the validation set. The hyperparameter tuning is made using [Optuna](https://github.com/optuna/optuna), an automatic hyperparameter optimization software framework, particularly designed for machine learning.\n",
    "\n",
    " - Assess the performance of the best-performing models on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spain\n",
    "param_Spain = parameters_search(df_Spain, 5, 5)\n",
    "param_Spain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA\n",
    "param_USA = parameters_search(df_USA, 5, 5)\n",
    "param_USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singapore\n",
    "param_Singapore = parameters_search(df_Singapore, 5, 5)\n",
    "param_Singapore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Germany\n",
    "param_Germany = parameters_search(df_Germany, 5, 5)\n",
    "param_Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japan\n",
    "param_Japan = parameters_search(df_Japan, 5, 5)\n",
    "param_Japan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let us use the optimal set of parameters to assess how well our models perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the best combination of features for each country\n",
    "train_Spain, valid_Spain, test_Spain = create_best_combination_dataset(df_Spain, 5,[2,3] )\n",
    "train_USA, valid_USA, test_USA = create_best_combination_dataset(df_USA, 1, [2,3,4,5])\n",
    "train_Singapore, valid_Singapore, test_Singapore = create_best_combination_dataset(df_Singapore, 2,[2,3,4] )\n",
    "train_Germany, valid_Germany, test_Germany = create_best_combination_dataset(df_Germany, 5,[2,3,4,5] )\n",
    "train_Japan, valid_Japan, test_Japan = create_best_combination_dataset(df_Japan,3 ,[2,3] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Spain, valid_Spain, test_Spain, plot_figures= True, plot_feature_importance= True, use_PCA= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_USA, valid_USA, test_USA, plot_figures= True, plot_feature_importance= True, use_PCA= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Singapore, valid_Singapore, test_Singapore, plot_figures= True, plot_feature_importance= True, use_PCA= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Germany, valid_Germany, test_Germany, plot_figures= True, plot_feature_importance= True, use_PCA= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Japan, valid_Japan, test_Japan, plot_figures= True, plot_feature_importance= True, use_PCA= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider incorporating additional features. At time $t$, we have information about both the attacks on the country and the status of all 255 sensors. By combining this data, we aim to enhance the predictive capabilities of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_all_sensors_data = True\n",
    "\n",
    "if add_all_sensors_data:\n",
    "\n",
    "    all_sensors_df = all_sensors_data(df)\n",
    "    # For Spain\n",
    "    train_Spain = merge_all_sensors(all_sensors_df, train_Spain, 3)\n",
    "    valid_Spain = merge_all_sensors(all_sensors_df, valid_Spain, 3)\n",
    "    test_Spain = merge_all_sensors(all_sensors_df, test_Spain, 3)\n",
    "\n",
    "    # For USA\n",
    "    train_USA = merge_all_sensors(all_sensors_df, train_USA, 3)\n",
    "    valid_USA = merge_all_sensors(all_sensors_df, valid_USA, 3)\n",
    "    test_USA = merge_all_sensors(all_sensors_df, test_USA, 3)\n",
    "\n",
    "    # For Singapore\n",
    "    train_Singapore = merge_all_sensors(all_sensors_df, train_Singapore, 3)\n",
    "    valid_Singapore = merge_all_sensors(all_sensors_df, valid_Singapore, 3)\n",
    "    test_Singapore = merge_all_sensors(all_sensors_df, test_Singapore, 3)\n",
    "\n",
    "    # For Germany\n",
    "    train_Germany = merge_all_sensors(all_sensors_df, train_Germany, 3)\n",
    "    valid_Germany = merge_all_sensors(all_sensors_df, valid_Germany, 3)\n",
    "    test_Germany = merge_all_sensors(all_sensors_df, test_Germany, 3)\n",
    "\n",
    "    # For Japan\n",
    "    train_Japan = merge_all_sensors(all_sensors_df, train_Japan, 3)\n",
    "    valid_Japan = merge_all_sensors(all_sensors_df, valid_Japan, 3)\n",
    "    test_Japan = merge_all_sensors(all_sensors_df, test_Japan, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Spain, valid_Spain, test_Spain, plot_figures= True, plot_feature_importance= False, use_PCA= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_USA, valid_USA, test_USA, plot_figures= True, plot_feature_importance= False, use_PCA= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Singapore, valid_Singapore, test_Singapore, plot_figures= True, plot_feature_importance= False, use_PCA= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Germany, valid_Germany, test_Germany, plot_figures= True, plot_feature_importance= False, use_PCA= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluate_models(train_Japan, valid_Japan, test_Japan, plot_figures= True, plot_feature_importance= False, use_PCA= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Section\n",
    "\n",
    "Let us attempt to provide a European perspective. Is it possible to forecast the quantity of cyberattacks expected in Europe for the upcoming month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us read European data and visualize it as a time series\n",
    "\n",
    "df_EU = select_continent(df, 'EU')\n",
    "\n",
    "daily_count_EU = visualize_ts(df_EU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the baseline dataset\n",
    "df_EU = create_baseline_dataset(daily_count_EU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best combination of parameters\n",
    "param_EU = parameters_search(df_EU, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best combination of features\n",
    "train_EU, valid_EU, test_EU = create_best_combination_dataset(df_EU, 5, [2,3,4,5])\n",
    "_ = evaluate_models(train_EU, valid_EU, test_EU, plot_figures= True, plot_feature_importance= True, use_PCA= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
