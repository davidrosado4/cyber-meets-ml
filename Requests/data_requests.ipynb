{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requests to API\n",
    "\n",
    "This python code serves as a base to perform requests to API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed imports\n",
    "import requests\n",
    "import json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String contains url/endpoint to perform a search operation on the index pattern\n",
    "url = \n",
    "# Header with Content-type required to make a http request\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "# Desire Query\n",
    "query = {\n",
    "    \"size\": 10000,\n",
    "    \"_source\": [\"commands\",\"startTime\", \"endTime\", \"hostIP\",\"loggedin\", \"peerIP\",\n",
    "                \"protocol\",\"hostPort\",\"peerPort\",\"geoip.country_name\",\"geoip.continent_code\",\n",
    "                \"hostGeoip.continent_code\",\"hostGeoip.country_name\",\"version\"],\n",
    "    \"query\": {\n",
    "        \"range\": {\n",
    "            \"startTime\": {\n",
    "                \"gte\": \"2023-05-01T00:00:00\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Needed function\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Flattens a nested dictionary into a flat dictionary with keys that are a combination of the original keys.\n",
    "\n",
    "    :param d: The input nested dictionary to be flattened.\n",
    "    :param parent_key: A string representing the prefix for the keys (used for recursion).\n",
    "    :param sep: The separator used to join keys when creating new flattened keys.\n",
    "    :return: A flat dictionary with flattened keys.\n",
    "    \"\"\"\n",
    "    flattened = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            flattened.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            flattened[new_key] = v\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_clustering(url,headers,query):\n",
    "    ''' \n",
    "    Perform a series of HTTP requests to a given URL with the provided headers and query parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to send the HTTP requests to.\n",
    "        headers (dict): The headers to include in the HTTP request.\n",
    "        query (dict): The query parameters for the request.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A concatenated DataFrame containing data from successful requests.\n",
    "        None: If any request is not successful (Status code other than 200).\n",
    "    '''\n",
    "\n",
    "    # List to store dataframes from each request\n",
    "    dfs = [] \n",
    "    # Initial start time\n",
    "    last_start_time = \"2023-05-01T00:00:00\"\n",
    "\n",
    "    # Define the target stop date\n",
    "    stop_date = datetime(2023, 7, 31, 0, 0, 0)\n",
    "    current_date = datetime.strptime(last_start_time, '%Y-%m-%dT%H:%M:%S')\n",
    "    current_day = None  # Track the current day\n",
    "\n",
    "    # Start loop request\n",
    "    while current_date<stop_date:\n",
    "        # Check if the current day has changed\n",
    "        if current_date.date() != current_day:\n",
    "            print(current_date)\n",
    "            current_day = current_date.date()  # Update current_day\n",
    "        # Update the start time for the query\n",
    "        query[\"query\"][\"range\"][\"startTime\"][\"gte\"] = last_start_time\n",
    "        # Perform the request\n",
    "        result = requests.get(url, auth=HTTPBasicAuth,\n",
    "                          headers=headers, data=json.dumps(query))\n",
    "        \n",
    "        if result.status_code == 200: # Successful\n",
    "\n",
    "            # Extract the data from the response\n",
    "            data = result.json()\n",
    "            # Filter the data\n",
    "            hits = [hit for hit in data['hits']['hits'] if hit['_source']['commands']]\n",
    "            # Find the maximum of startTime\n",
    "            last_start_time = max(hit['_source']['startTime'] for hit in hits)\n",
    "            current_date = datetime.strptime(last_start_time, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            # Modify the data to remove specific columns\n",
    "            for hit in hits:\n",
    "                hit.pop('_index', None)\n",
    "                hit.pop('_type', None)\n",
    "                hit.pop('_id', None)\n",
    "                hit.pop('_score', None)\n",
    "            # Flatten dicts to convert data to desired format\n",
    "            flattened_data_list = [flatten_dict(record) for record in hits]\n",
    "            # Convert to a dataframe\n",
    "            df = pd.DataFrame(flattened_data_list)\n",
    "            # Append dataframe\n",
    "            dfs.append(df)\n",
    "\n",
    "        else: # Not succesful\n",
    "            print(\"Request was not successful. Status code:\", result.status_code)\n",
    "            return None\n",
    "        \n",
    "    # Concatenate all dataframes into one\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request the data\n",
    "final_df =request_to_clustering(url,headers,query)    \n",
    "\n",
    "# Store data\n",
    "final_df.to_csv('../Data/Cluster_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String contains url/endpoint to perform a search operation on the index pattern\n",
    "url = \n",
    "# Header with Content-type required to make a http request\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "# Desire Query\n",
    "query = {\n",
    "    \"size\": 10000,\n",
    "    \"_source\": [\"commands\",\"startTime\", \"endTime\", \"hostIP\",\"loggedin\", \"peerIP\",\n",
    "                \"protocol\",\"hostPort\",\"peerPort\",\"geoip.country_name\",\"geoip.continent_code\",\n",
    "                \"hostGeoip.continent_code\",\"hostGeoip.country_name\",\"version\"],\n",
    "    \"query\": {\n",
    "        \"range\": {\n",
    "            \"startTime\": {\n",
    "                \"gte\": \"2023-05-01T00:00:00\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "# Needed function\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    \"\"\"\n",
    "    Flattens a nested dictionary into a flat dictionary with keys that are a combination of the original keys.\n",
    "\n",
    "    :param d: The input nested dictionary to be flattened.\n",
    "    :param parent_key: A string representing the prefix for the keys (used for recursion).\n",
    "    :param sep: The separator used to join keys when creating new flattened keys.\n",
    "    :return: A flat dictionary with flattened keys.\n",
    "    \"\"\"\n",
    "    flattened = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            flattened.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            flattened[new_key] = v\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_forecasting(url,headers,query,initial_time,final_year,final_month,final_day):\n",
    "    ''' \n",
    "    Perform a series of HTTP requests to a given URL with the provided headers and query parameters.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to send the HTTP requests to.\n",
    "        headers (dict): The headers to include in the HTTP request.\n",
    "        query (dict): The query parameters for the request.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A concatenated DataFrame containing data from successful requests.\n",
    "        None: If any request is not successful (Status code other than 200).\n",
    "    '''\n",
    "\n",
    "    # List to store dataframes from each request\n",
    "    dfs = [] \n",
    "    # Initial start time\n",
    "    last_start_time = initial_time\n",
    "\n",
    "    # Define the target stop date\n",
    "    stop_date = datetime(final_year, final_month, final_day, 0, 0, 0)\n",
    "    current_date = datetime.strptime(last_start_time, '%Y-%m-%dT%H:%M:%S')\n",
    "    current_day = None  # Track the current day\n",
    "\n",
    "    # Start loop request\n",
    "    while current_date<stop_date:\n",
    "        # Check if the current day has changed\n",
    "        if current_date.date() != current_day:\n",
    "            print(current_date)\n",
    "            current_day = current_date.date()  # Update current_day\n",
    "        # Update the start time for the query\n",
    "        query[\"query\"][\"range\"][\"startTime\"][\"gte\"] = last_start_time\n",
    "        # Perform the request\n",
    "        result = requests.get(url, auth=HTTPBasicAuth,\n",
    "                          headers=headers, data=json.dumps(query))\n",
    "        \n",
    "        if result.status_code == 200: # Successful\n",
    "\n",
    "            # Extract the data from the response\n",
    "            data = result.json()\n",
    "            # Filter the data\n",
    "            hits = [hit for hit in data['hits']['hits'] if hit['_source']['commands']]\n",
    "            # Find the maximum of startTime\n",
    "            last_start_time = max(hit['_source']['startTime'] for hit in hits)\n",
    "            current_date = datetime.strptime(last_start_time, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "            # Modify the data to remove specific columns\n",
    "            for hit in hits:\n",
    "                hit.pop('_index', None)\n",
    "                hit.pop('_type', None)\n",
    "                hit.pop('_id', None)\n",
    "                hit.pop('_score', None)\n",
    "            # Flatten dicts to convert data to desired format\n",
    "            flattened_data_list = [flatten_dict(record) for record in hits]\n",
    "            # Convert to a dataframe\n",
    "            df = pd.DataFrame(flattened_data_list)\n",
    "            # Append dataframe\n",
    "            dfs.append(df)\n",
    "\n",
    "        else: # Not succesful\n",
    "            print(\"Request was not successful. Status code:\", result.status_code)\n",
    "            return None\n",
    "        \n",
    "    # Concatenate all dataframes into one\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df21_november_to_april = request_to_forecasting(url,headers,query,\"2021-11-30T00:00:00\",2022,4,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store csv in Data folder\n",
    "df21_november_to_april.to_csv('../Data/21_november_to_april.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22_april_to_november = request_to_forecasting(url,headers,query,\"2022-04-30T00:00:00\",2022,11,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store csv in Data folder\n",
    "df22_april_to_november.to_csv('../Data/22_april_to_november.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22_november_to_april = request_to_forecasting(url,headers,query,\"2022-11-30T00:00:00\",2023,4,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store csv in Data folder\n",
    "df22_november_to_april.to_csv('../Data/22_november_to_april.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df23_april_to_november = request_to_forecasting(url,headers,query,\"2023-04-30T00:00:00\",2023,11,29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store csv in Data folder\n",
    "df23_april_to_november.to_csv('../Data/23_april_to_november.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
